From f7c1b3e6d378f8a3d84c8770bd2c2b386ecca3ea Mon Sep 17 00:00:00 2001
From: Peter Vary <pvary@cloudera.com>
Date: Tue, 17 Oct 2017 11:30:33 +0200
Subject: [PATCH 1307/1363] CLOUDERA-BUILD: CDH-54574 Testcluster should have
 specific settings

Change-Id: I68e1708bbb2ebc29cb4c8a9548900fe12ec70cb4
---
 cloudera/beeline/hive-beeline.json            |   20 +
 cloudera/beeline/jenkins_beehive.sh           |  117 ++++--
 cloudera/beeline/testconfiguration.properties |  483 ++++++++++++++++++++++++-
 3 files changed, 578 insertions(+), 42 deletions(-)
 create mode 100644 cloudera/beeline/hive-beeline.json

diff --git a/cloudera/beeline/hive-beeline.json b/cloudera/beeline/hive-beeline.json
new file mode 100644
index 0000000..cb60322
--- /dev/null
+++ b/cloudera/beeline/hive-beeline.json
@@ -0,0 +1,20 @@
+{
+  "HIVE": {
+    "HIVESERVER2": {
+      "hive_hs2_config_safety_valve": "<property><name>hive.exec.pre.hooks</name><value>org.apache.hadoop.hive.ql.hooks.PreExecutePrinter</value></property><property><name>hive.exec.post.hooks</name><value>org.apache.hadoop.hive.ql.hooks.PostExecutePrinter</value></property><property><name>hive.in.test</name><value>true</value></property><property><name>hive.async.log.enabled</name><value>false</value></property><property><name>hive.ignore.mapjoin.hint</name><value>false</value></property><property><name>hive.fetch.task.conversion</name><value>minimal</value></property><property><name>hive.materializedview.rewriting</name><value>true</value></property><property><name>hive.mapred.local.mem</name><value>256</value></property>",
+      "hiveserver2_enable_cbo": "true",
+      "hiveserver2_stats_fetch_column_stats": "false",
+      "hiveserver2_limit_pushdown_memory_usage": "0",
+      "hiveserver2_optimize_index_filter": "false",
+      "hiveserver2_enable_mapjoin": "false",
+      "hiveserver2_java_heapsize":"1063741824",
+      "hive_hs2_env_safety_valve":"HIVE_LOCAL_TASK_CHILD_OPTS=\"-Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -XX:MaxPermSize=512M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:OnOutOfMemoryError=/usr/lib64/cmf/service/common/killparent.sh\""
+    },
+    "HIVEMETASTORE": {
+      "metastore_canary_health_enabled": "false"
+    },
+    "SERVICE": {
+      "hive_aux_jars_path_dir": "/home/systest"
+    }
+  }
+}
diff --git a/cloudera/beeline/jenkins_beehive.sh b/cloudera/beeline/jenkins_beehive.sh
index 6038c5f..6fb7234 100644
--- a/cloudera/beeline/jenkins_beehive.sh
+++ b/cloudera/beeline/jenkins_beehive.sh
@@ -48,47 +48,22 @@ if [ "$NEW_CLUSTER" = true ]
 then
   # Create the hive safety valves
   OPTIONAL_ARGS="-is=HDFS,YARN,ZOOKEEPER,MAPREDUCE,HIVE,SPARK,SPARK_ON_YARN"
-  OPTIONAL_ARGS="${OPTIONAL_ARGS} -jsonurl http://github.mtv.cloudera.com/raw/pvary/notebook/master/hive-beeline/hive-beeline.json"
+  OPTIONAL_ARGS="${OPTIONAL_ARGS} -jsonurl http://github.mtv.cloudera.com/raw/CDH/hive/cdh5-1.1.0/cloudera/beeline/hive-beeline.json"
 
   # Setup the cluster
   cloudcat_setup
-
-  # Create beeline user 'user' and home directory
-  # Is this enough?
-  for CLUSTER_NODE in ${HOSTS_LIST[@]}; do
-    ssh -o UserKnownHostsFile=/dev/null \
-        -o StrictHostKeyChecking=no -q \
-        ${SSH_USER}@${CLUSTER_NODE}.${DOMAIN} \
-        "sudo useradd -m user"
-  done
-
-  ssh -o UserKnownHostsFile=/dev/null \
-      -o StrictHostKeyChecking=no -q \
-          ${SSH_USER}@${HIVESERVER2_NODE} << __EOF
-      sudo -u hdfs hdfs dfs -mkdir /user/user
-      sudo -u hdfs hdfs dfs -chown user:user /user/user
-__EOF
 fi
 
-BEELINE_USER=user
+BEELINE_USER=hive
 BEELINE_PASSWORD=
 
 DATA_DIR=/run/cloudera-scm-agent
-AUX_DIR=/tmp/aux
-
-# Apply patch
-if [[ -s patch.file ]]
-then
-  git apply -3 -p0 patch.file
-  echo "Patch applied"
-else
-  echo "No patch file to apply"
-fi
+AUX_DIR=/home/systest
 
 # Compiling hive
 echo "Compiling hive..."
 cd $WORKSPACE
-mvn clean install -DskipTests -Phadoop-2
+mvn clean install -DskipTests -Phadoop-2,dist
 echo "Compiling itests..."
 cd $WORKSPACE/itests
 mvn clean install -DskipTests -Phadoop-2
@@ -101,21 +76,59 @@ scp -o UserKnownHostsFile=/dev/null \
     data.tar ${SSH_USER}@${HIVESERVER2_NODE}:/tmp/
 ssh -o UserKnownHostsFile=/dev/null \
     -o StrictHostKeyChecking=no -q \
-	${SSH_USER}@${HIVESERVER2_NODE} << __EOF
+    ${SSH_USER}@${HIVESERVER2_NODE} << __EOF
     sudo su -
     mkdir -p ${DATA_DIR}
     cd ${DATA_DIR}
     tar -xf /tmp/data.tar
-    mkdir -p ${AUX_DIR}
-    chmod a+rwx ${AUX_DIR}
     chown systest:systest -R ${DATA_DIR}/data
     chmod a+rw -R ${DATA_DIR}/data
 __EOF
 
-# Upload hive-it-util-*.jar collection to the target machine
-scp -o UserKnownHostsFile=/dev/null \
-    -o StrictHostKeyChecking=no \
-    $WORKSPACE/itests/util/target/hive-it-util-*.jar ${SSH_USER}@${HIVESERVER2_NODE}:${AUX_DIR}
+
+for CLUSTER_NODE in "${HOSTS_LIST[@]}"
+do
+  # Upload hive-*.jar collection to the target machines
+  ssh -o UserKnownHostsFile=/dev/null \
+      -o StrictHostKeyChecking=no -q \
+      ${SSH_USER}@${CLUSTER_NODE} << __EOF
+      mkdir -p /tmp/hive-jars
+      sudo chmod a+rwx ${AUX_DIR}
+__EOF
+  scp -o UserKnownHostsFile=/dev/null \
+      -o StrictHostKeyChecking=no \
+      $WORKSPACE/packaging/target/apache-hive-*-bin/apache-hive-*-bin/lib/hive*jar ${SSH_USER}@${CLUSTER_NODE}:/tmp/hive-jars
+  ssh -o UserKnownHostsFile=/dev/null \
+      -o StrictHostKeyChecking=no -q \
+        ${SSH_USER}@${CLUSTER_NODE} \
+      sudo cp /tmp/hive-jars/* /opt/cloudera/parcels/CDH/jars
+
+  # Upload hive-it-util-*.jar to the target machines
+  scp -o UserKnownHostsFile=/dev/null \
+      -o StrictHostKeyChecking=no \
+      $WORKSPACE/itests/util/target/hive-it-util-*.jar ${SSH_USER}@${CLUSTER_NODE}:${AUX_DIR}
+done
+
+# Restart cluster to read aux jars
+set +e
+echo "Restarting HIVE service"
+WAIT_TIME=300
+WAIT_TIME_DECREMENT=5
+curl -X POST -u "admin:admin" -i http://${HIVESERVER2_NODE}:7180/api/v17/clusters/Cluster%201/services/HIVE-1/commands/restart
+sleep $WAIT_TIME_DECREMENT
+echo
+echo "Waiting until HIVE service is ready"
+timeout --foreground 0.5 bash -c "echo >\"/dev/tcp/${HIVESERVER2_NODE}/10000\"" >&/dev/null
+PORT_CHECK=$?
+while [ "$PORT_CHECK" -ne 0 ] && [ $WAIT_TIME -ge 0 ]
+do
+  echo "Sleeping ${WAIT_TIME_DECREMENT}s"
+  sleep 5
+  (( WAIT_TIME-=$WAIT_TIME_DECREMENT ))
+  timeout --foreground 0.5 bash -c "echo >\"/dev/tcp/${HIVESERVER2_NODE}/10000\"" >&/dev/null
+  PORT_CHECK=$?
+done
+set -e
 
 # Load the property file, so we will no which tests to run
 cd $WORKSPACE/cloudera/beeline
@@ -129,7 +142,21 @@ then
   echo "Running parallel qtests..."
   cd $WORKSPACE/itests/qtest
   set +e
-  mvn clean test -Phadoop-2 -Dtest=TestBeeLineDriver -Dtest.beeline.url="jdbc:hive2://${HIVESERVER2_NODE}:10000" -Dtest.data.dir="${DATA_DIR}/data/files" -Dtest.beeline.user="${BEELINE_USER}" -Dtest.beeline.password="${BEELINE_PASSWORD}" -Dmaven.test.redirectTestOutputToFile=true -Djunit.parallel.timeout=300 -Dqfile="${beeline_parallel}"
+  mvn clean test \
+    -Phadoop-2 \
+    -Dtest=TestBeeLineDriver \
+    -Dtest.beeline.url="jdbc:hive2://${HIVESERVER2_NODE}:10000" \
+    -Dtest.data.dir="${DATA_DIR}/data/files" \
+    -Dtest.beeline.user="${BEELINE_USER}" \
+    -Dtest.beeline.password="${BEELINE_PASSWORD}" \
+    -Dmaven.test.redirectTestOutputToFile=true \
+    -Djunit.parallel.timeout=3000 \
+    -Dtest.results.dir=ql/src/test/results/clientpositive \
+    -Dtest.init.script=q_test_init.sql \
+    -Dtest.beeline.compare.portable=true \
+    -Dtest.beeline.shared.database=false \
+    -Djunit.parallel.threads=10 \
+    -Dqfile="${beeline_parallel}"
   TEST_RESULT=$?
   set -e
   rm -rf target.parallel
@@ -142,7 +169,21 @@ if [ -n "${beeline_sequential}" ]
 then
   echo "Running sequential qtests..."
   set +e
-  mvn clean test -Phadoop-2 -Dtest=TestBeeLineDriver -Dtest.beeline.url="jdbc:hive2://${HIVESERVER2_NODE}:10000" -Dtest.data.dir="${DATA_DIR}/data/files" -Dtest.beeline.user="${BEELINE_USER}" -Dtest.beeline.password="${BEELINE_PASSWORD}" -Dmaven.test.redirectTestOutputToFile=true -Djunit.parallel.timeout=300 -Dqfile="${beeline_sequential}" -Djunit.parallel.threads=1
+  mvn clean test \
+    -Phadoop-2 \
+    -Dtest=TestBeeLineDriver \
+    -Dtest.beeline.url="jdbc:hive2://${HIVESERVER2_NODE}:10000" \
+    -Dtest.data.dir="${DATA_DIR}/data/files" \
+    -Dtest.beeline.user="${BEELINE_USER}" \
+    -Dtest.beeline.password="${BEELINE_PASSWORD}" \
+    -Dmaven.test.redirectTestOutputToFile=true \
+    -Djunit.parallel.timeout=3000 \
+    -Dtest.results.dir=ql/src/test/results/clientpositive \
+    -Dtest.init.script=q_test_init.sql \
+    -Dtest.beeline.compare.portable=true \
+    -Dtest.beeline.shared.database=true \
+    -Djunit.parallel.threads=1 \
+    -Dqfile="${beeline_sequential}"
   TEST_RESULT=$?
   rm -rf target.sequential
   mv target target.sequential
diff --git a/cloudera/beeline/testconfiguration.properties b/cloudera/beeline/testconfiguration.properties
index b66e71e..9322730 100644
--- a/cloudera/beeline/testconfiguration.properties
+++ b/cloudera/beeline/testconfiguration.properties
@@ -1,5 +1,480 @@
 beeline.parallel=create_merge_compressed.q,\
-  udf_unix_timestamp.q
-
-beeline.sequential=escape_comments.q,\
-  select_dummy_source.q
+  udf_unix_timestamp.q,\
+add_part_multiple.q,\
+alter2.q,\
+alter3.q,\
+alter4.q,\
+alter5.q,\
+alter_char1.q,\
+alter_char2.q,\
+alter_file_format.q,\
+alter_merge.q,\
+alter_merge_2.q,\
+alter_merge_stats.q,\
+alter_numbuckets_partitioned_table2_h23.q,\
+alter_numbuckets_partitioned_table_h23.q,\
+alter_partition_change_col.q,\
+alter_partition_clusterby_sortby.q,\
+alter_partition_coltype.q,\
+alter_partition_protect_mode.q,\
+alter_partition_with_whitelist.q,\
+alter_rename_partition.q,\
+alter_skewed_table.q,\
+alter_table_cascade.q,\
+alter_table_not_sorted.q,\
+alter_table_partition_drop.q,\
+alter_table_serde.q,\
+alter_table_serde2.q,\
+alter_varchar1.q,\
+alter_varchar2.q,\
+alter_view_as_select.q,\
+alter_view_rename.q,\
+auto_join10.q,\
+auto_join16.q,\
+auto_join24.q,\
+auto_join26.q,\
+auto_join27.q,\
+auto_join30.q,\
+auto_join31.q,\
+auto_join32.q,\
+auto_join33.q,\
+auto_join_filters.q,\
+auto_join_nulls.q,\
+auto_sortmerge_join_1.q,\
+auto_sortmerge_join_10.q,\
+auto_sortmerge_join_11.q,\
+auto_sortmerge_join_12.q,\
+auto_sortmerge_join_13.q,\
+auto_sortmerge_join_14.q,\
+auto_sortmerge_join_15.q,\
+auto_sortmerge_join_16.q,\
+auto_sortmerge_join_3.q,\
+auto_sortmerge_join_4.q,\
+auto_sortmerge_join_5.q,\
+auto_sortmerge_join_6.q,\
+auto_sortmerge_join_7.q,\
+auto_sortmerge_join_8.q,\
+avro_add_column.q,\
+avro_add_column2.q,\
+avro_add_column3.q,\
+avro_change_schema.q,\
+avro_charvarchar.q,\
+avro_compression_enabled.q,\
+avro_compression_enabled_native.q,\
+avro_date.q,\
+avro_decimal.q,\
+avro_decimal_native.q,\
+avro_decimal_old.q,\
+avro_deserialize_map_null.q,\
+avro_evolved_schemas.q,\
+avro_joins.q,\
+avro_joins_native.q,\
+avro_native.q,\
+avro_nullable_union.q,\
+avro_partitioned_native.q,\
+avro_sanity_test.q,\
+avro_schema_evolution_native.q,\
+avro_schema_literal.q,\
+avro_type_evolution.q,\
+avrocountemptytbl.q,\
+bucket1.q,\
+bucket2.q,\
+bucket3.q,\
+bucket6.q,\
+bucket_groupby.q,\
+bucket_map_join_1.q,\
+bucket_map_join_2.q,\
+bucketcontext_1.q,\
+bucketcontext_2.q,\
+bucketcontext_3.q,\
+bucketcontext_4.q,\
+bucketcontext_5.q,\
+bucketcontext_6.q,\
+bucketcontext_7.q,\
+bucketcontext_8.q,\
+bucketizedhiveinputformat_auto.q,\
+bucketmapjoin10.q,\
+bucketmapjoin11.q,\
+bucketmapjoin12.q,\
+bucketmapjoin13.q,\
+bucketmapjoin6.q,\
+bucketmapjoin7.q,\
+bucketmapjoin8.q,\
+bucketmapjoin9.q,\
+bucketmapjoin_negative3.q,\
+char_join1.q,\
+cluster.q,\
+columnstats_partlvl_dp.q,\
+compute_stats_binary.q,\
+compute_stats_boolean.q,\
+compute_stats_decimal.q,\
+compute_stats_double.q,\
+compute_stats_empty_table.q,\
+compute_stats_long.q,\
+compute_stats_string.q,\
+constprog_semijoin.q,\
+create_like_view.q,\
+create_merge_compressed.q,\
+create_view_defaultformats.q,\
+create_view_partitioned.q,\
+cross_join.q,\
+database_location.q,\
+date_join1.q,\
+dbtxnmgr_compact1.q,\
+dbtxnmgr_compact2.q,\
+dbtxnmgr_compact3.q,\
+describe_database_json.q,\
+describe_table_json.q,\
+distinct_stats.q,\
+drop_multi_partitions.q,\
+drop_partition_with_stats.q,\
+drop_partitions_filter.q,\
+drop_partitions_filter2.q,\
+drop_partitions_filter3.q,\
+drop_partitions_ignore_protection.q,\
+dynpart_merge.q,\
+fileformat_sequencefile.q,\
+groupby12.q,\
+groupby1_limit.q,\
+groupby1_map.q,\
+groupby1_map_nomap.q,\
+groupby1_map_skew.q,\
+groupby1_noskew.q,\
+groupby2.q,\
+groupby3_map.q,\
+groupby3_map_multi_distinct.q,\
+groupby3_map_skew.q,\
+groupby3_noskew.q,\
+groupby3_noskew_multi_distinct.q,\
+groupby7.q,\
+groupby7_map.q,\
+groupby7_map_skew.q,\
+groupby7_noskew.q,\
+groupby9.q,\
+groupby_complex_types.q,\
+groupby_duplicate_key.q,\
+groupby_grouping_id1.q,\
+groupby_grouping_id2.q,\
+groupby_grouping_sets1.q,\
+groupby_grouping_sets4.q,\
+groupby_multi_insert_common_distinct.q,\
+groupby_sort_1_23.q,\
+groupby_sort_2.q,\
+groupby_sort_skew_1_23.q,\
+infer_bucket_sort_list_bucket.q,\
+infer_bucket_sort_multi_insert.q,\
+input.q,\
+input0.q,\
+input11_limit.q,\
+input14.q,\
+input17.q,\
+input18.q,\
+input1_limit.q,\
+input2.q,\
+input20.q,\
+input21.q,\
+input22.q,\
+input23.q,\
+input24.q,\
+input25.q,\
+input26.q,\
+input28.q,\
+input3.q,\
+input33.q,\
+input39.q,\
+input3_limit.q,\
+input4.q,\
+input41.q,\
+input42.q,\
+input46.q,\
+input5.q,\
+input_columnarserde.q,\
+input_lazyserde.q,\
+input_part10.q,\
+input_part3.q,\
+input_part7.q,\
+input_part9.q,\
+insert_into_with_schema.q,\
+join24.q,\
+join29.q,\
+join31.q,\
+join32.q,\
+join34.q,\
+join38.q,\
+join_1to1.q,\
+join_filters.q,\
+join_grp_diff_keys.q,\
+join_merging.q,\
+join_nulls.q,\
+join_rc.q,\
+join_view.q,\
+leftsemijoin.q,\
+leftsemijoin_mr.q,\
+louter_join_ppr.q,\
+mapjoin1.q,\
+mapjoin_distinct.q,\
+mapjoin_filter_on_outerjoin.q,\
+mapjoin_subquery.q,\
+mapjoin_subquery2.q,\
+mapjoin_test_outer.q,\
+merge_join_1.q,\
+mergejoin.q,\
+mergejoins.q,\
+mergejoins_mixed.q,\
+multi_insert_gby.q,\
+multi_insert_gby2.q,\
+multi_insert_gby3.q,\
+multi_insert_mixed.q,\
+multi_insert_union_src.q,\
+multi_insert_with_join.q,\
+multi_join_union.q,\
+optional_outer.q,\
+orc_create.q,\
+outer_join_ppr.q,\
+parquet_array_map_emptynullvals.q,\
+parquet_array_null_element.q,\
+parquet_array_of_multi_field_struct.q,\
+parquet_array_of_optional_elements.q,\
+parquet_array_of_required_elements.q,\
+parquet_array_of_single_field_struct.q,\
+parquet_array_of_structs.q,\
+parquet_array_of_unannotated_groups.q,\
+parquet_array_of_unannotated_primitives.q,\
+parquet_avro_array_of_primitives.q,\
+parquet_avro_array_of_single_field_struct.q,\
+parquet_create.q,\
+parquet_ctas.q,\
+parquet_decimal.q,\
+parquet_decimal1.q,\
+parquet_external_time.q,\
+parquet_join.q,\
+parquet_map_null.q,\
+parquet_map_of_arrays_of_ints.q,\
+parquet_map_of_maps.q,\
+parquet_mixed_case.q,\
+parquet_nested_complex.q,\
+parquet_ppd.q,\
+parquet_ppd_multifiles.q,\
+parquet_ppd_partition.q,\
+parquet_read_backward_compatible_files.q,\
+parquet_table_with_subschema.q,\
+parquet_thrift_array_of_primitives.q,\
+parquet_thrift_array_of_single_field_struct.q,\
+parquet_write_correct_definition_levels.q,\
+partInit.q,\
+partcols1.q,\
+partition_boolexpr.q,\
+partition_char.q,\
+partition_date.q,\
+partition_decode_name.q,\
+partition_multilevels.q,\
+partition_schema1.q,\
+partition_serde_format.q,\
+partition_special_char.q,\
+partition_type_check.q,\
+partition_type_in_plan.q,\
+partition_varchar1.q,\
+partition_varchar2.q,\
+partition_vs_table_metadata.q,\
+partition_wise_fileformat.q,\
+partition_wise_fileformat10.q,\
+partition_wise_fileformat11.q,\
+partition_wise_fileformat12.q,\
+partition_wise_fileformat13.q,\
+partition_wise_fileformat14.q,\
+partition_wise_fileformat15.q,\
+partition_wise_fileformat16.q,\
+partition_wise_fileformat2.q,\
+partition_wise_fileformat4.q,\
+partition_wise_fileformat5.q,\
+partition_wise_fileformat6.q,\
+partition_wise_fileformat7.q,\
+partition_wise_fileformat8.q,\
+partition_wise_fileformat9.q,\
+partitions_json.q,\
+protectmode.q,\
+protectmode2.q,\
+rcfile_bigdata.q,\
+rcfile_columnar.q,\
+rcfile_lazydecompress.q,\
+rcfile_merge2.q,\
+rcfile_merge3.q,\
+rcfile_merge4.q,\
+rcfile_toleratecorruptions.q,\
+rcfile_union.q,\
+rename_column.q,\
+selectDistinctStar.q,\
+semijoin.q,\
+show_create_table_db_table.q,\
+show_create_table_partitioned.q,\
+show_create_table_serde.q,\
+show_functions.q,\
+showparts.q,\
+skewjoin_mapjoin1.q,\
+skewjoin_mapjoin10.q,\
+skewjoin_mapjoin11.q,\
+skewjoin_mapjoin2.q,\
+skewjoin_mapjoin3.q,\
+skewjoin_mapjoin4.q,\
+skewjoin_mapjoin5.q,\
+skewjoin_mapjoin6.q,\
+skewjoin_mapjoin7.q,\
+skewjoin_mapjoin8.q,\
+skewjoin_mapjoin9.q,\
+skewjoin_noskew.q,\
+skewjoin_union_remove_1.q,\
+skewjoin_union_remove_2.q,\
+smb_join_partition_key.q,\
+smb_mapjoin_1.q,\
+smb_mapjoin_10.q,\
+smb_mapjoin_11.q,\
+smb_mapjoin_12.q,\
+smb_mapjoin_13.q,\
+smb_mapjoin_14.q,\
+smb_mapjoin_15.q,\
+smb_mapjoin_16.q,\
+smb_mapjoin_17.q,\
+smb_mapjoin_18.q,\
+smb_mapjoin_19.q,\
+smb_mapjoin_2.q,\
+smb_mapjoin_20.q,\
+smb_mapjoin_21.q,\
+smb_mapjoin_22.q,\
+smb_mapjoin_25.q,\
+smb_mapjoin_3.q,\
+smb_mapjoin_4.q,\
+smb_mapjoin_5.q,\
+stats1.q,\
+stats10.q,\
+stats14.q,\
+stats15.q,\
+stats16.q,\
+stats18.q,\
+stats20.q,\
+stats3.q,\
+stats8.q,\
+stats9.q,\
+stats_based_fetch_decision.q,\
+stats_empty_partition.q,\
+stats_invalidation.q,\
+stats_list_bucket.q,\
+stats_partscan_1_23.q,\
+statsfs.q,\
+subq2.q,\
+subquery_exists.q,\
+subquery_exists_having.q,\
+subquery_in.q,\
+subquery_in_having.q,\
+subquery_notexists.q,\
+subquery_notexists_having.q,\
+subquery_notin.q,\
+subquery_notin_having.q,\
+subquery_unqualcolumnrefs.q,\
+subquery_views.q,\
+temp_table_precedence.q,\
+truncate_column_buckets.q,\
+truncate_table.q,\
+union14.q,\
+union17.q,\
+union2.q,\
+union23.q,\
+union25.q,\
+union26.q,\
+union3.q,\
+union31.q,\
+union32.q,\
+union34.q,\
+union5.q,\
+union7.q,\
+union_date.q,\
+union_lateralview.q,\
+union_null.q,\
+union_remove_1.q,\
+union_remove_2.q,\
+union_remove_20.q,\
+union_remove_3.q,\
+union_remove_6.q,\
+union_remove_6_subq.q,\
+union_remove_7.q,\
+union_remove_8.q,\
+unionall_join_nullconstant.q
+beeline.sequential=alter_rename_table.q,\
+archive_multi.q,\
+auto_join0.q,\
+auto_join1.q,\
+auto_join11.q,\
+auto_join12.q,\
+auto_join13.q,\
+auto_join15.q,\
+auto_join17.q,\
+auto_join18.q,\
+auto_join18_multi_distinct.q,\
+auto_join19.q,\
+auto_join2.q,\
+auto_join20.q,\
+auto_join21.q,\
+auto_join22.q,\
+auto_join23.q,\
+auto_join28.q,\
+auto_join29.q,\
+auto_join3.q,\
+auto_join4.q,\
+auto_join5.q,\
+auto_join6.q,\
+auto_join7.q,\
+auto_join8.q,\
+auto_join9.q,\
+auto_sortmerge_join_9.q,\
+columnstats_part_coltype.q,\
+columnstats_partlvl.q,\
+columnstats_tbllvl.q,\
+create_func1.q,\
+database.q,\
+describe_database.q,\
+describe_syntax.q,\
+describe_table.q,\
+display_colstats_tbllvl.q,\
+groupby_map_ppr.q,\
+groupby_position.q,\
+innerjoin.q,\
+join0.q,\
+join1.q,\
+join13.q,\
+join15.q,\
+join18.q,\
+join18_multi_distinct.q,\
+join2.q,\
+join21.q,\
+join23.q,\
+join3.q,\
+join4.q,\
+join40.q,\
+join41.q,\
+join5.q,\
+join6.q,\
+join7.q,\
+join8.q,\
+mapjoin_mapjoin.q,\
+mapjoin_memcheck.q,\
+parallel_join0.q,\
+parallel_join1.q,\
+parquet_ppd_boolean.q,\
+parquet_ppd_char.q,\
+parquet_ppd_decimal.q,\
+parquet_ppd_varchar.q,\
+parquet_serde.q,\
+rcfile_createas1.q,\
+rcfile_merge1.q,\
+rcfile_null_value.q,\
+show_indexes_edge_cases.q,\
+show_indexes_syntax.q,\
+show_partitions.q,\
+show_tables.q,\
+show_tablestatus.q,\
+skewjoin.q,\
+temp_table.q,\
+truncate_column.q,\
+truncate_column_list_bucket.q,\
+union16.q,\
+union_view.q,\
+view.q
-- 
1.7.9.5

