From d4f9ed351067b5584df66406b46170e7dbc7efec Mon Sep 17 00:00:00 2001
From: Janaki Lahorani <janaki@cloudera.com>
Date: Wed, 19 Jul 2017 08:59:40 -0700
Subject: [PATCH 1202/1363] CDH-39630: HIVE-16960: Hive throws an ugly error
 exception when HDFS sticky bit is set (Janaki
 Lahorani, reviewed by Sahil Takiar)

Change-Id: I59bf10b8462a30bd5a36fe4bb0d34192dc0dd060
---
 .../apache/hive/service/TestDFSErrorHandling.java  |  152 ++++++++++++++++++++
 ql/src/java/org/apache/hadoop/hive/ql/Driver.java  |   11 ++
 .../java/org/apache/hadoop/hive/ql/ErrorMsg.java   |   35 +++++
 .../org/apache/hadoop/hive/ql/exec/MoveTask.java   |   21 ++-
 .../org/apache/hadoop/hive/ql/metadata/Hive.java   |  125 ++++++++++++----
 .../hadoop/hive/ql/metadata/HiveException.java     |   30 +++-
 .../hive/service/cli/thrift/ThriftCLIService.java  |    5 +-
 7 files changed, 344 insertions(+), 35 deletions(-)
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/service/TestDFSErrorHandling.java

diff --git a/itests/hive-unit/src/test/java/org/apache/hive/service/TestDFSErrorHandling.java b/itests/hive-unit/src/test/java/org/apache/hive/service/TestDFSErrorHandling.java
new file mode 100644
index 0000000..c58767f
--- /dev/null
+++ b/itests/hive-unit/src/test/java/org/apache/hive/service/TestDFSErrorHandling.java
@@ -0,0 +1,152 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.ErrorMsg;
+import org.apache.hadoop.hive.shims.HadoopShims.MiniDFSShim;
+import org.apache.hive.jdbc.miniHS2.MiniHS2;
+import org.apache.hive.service.cli.HiveSQLException;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.util.HashMap;
+
+/**
+ * If the operation fails because of a DFS error, it used to result in an ugly stack at the client.
+ * HIVE-16960 fixes that issue.  This test case checks one DFS error related to sticky bit.  When
+ * the sticky bit is set, a user error indicating access denied will the thrown.
+ *
+ * Setup: HIVE_SERVER2_ENABLE_DOAS set to true:  HS2 performs the operation as connected user.
+ * Connect to HS2 as "hive".
+ * Create a file and set the sticky bit on the directory.  This will not allow the file to move
+ * out of the directory.
+ * Perform "LOAD" operation.  This operation will attempt to move the file, resulting in an error
+ * from DFS.  The DFS error will translate to an Hive Error with number 20009, that corresponds to
+ * "ACCESS DENIED".  The test checks that 20009 is thrown.
+ *
+ * Additional tests can be added to cover Quota related exceptions.
+ */
+public class TestDFSErrorHandling
+{
+
+  private static MiniHS2 miniHS2 = null;
+  private static HiveConf hiveConf = null;
+
+  @BeforeClass
+  public static void startServices() throws Exception {
+    hiveConf = new HiveConf();
+    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MIN_WORKER_THREADS, 1);
+    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MAX_WORKER_THREADS, 1);
+    hiveConf.setBoolVar(ConfVars.METASTORE_EXECUTE_SET_UGI, true);
+    hiveConf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
+
+    // Setting hive.server2.enable.doAs to True ensures that HS2 performs the query operation as
+    // the connected user instead of the user running HS2.
+    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, true);
+
+    miniHS2 = new MiniHS2.Builder()
+        .withMiniMR()
+        .withRemoteMetastore()
+        .withConf(hiveConf).build();
+
+    miniHS2.start(new HashMap<String, String>());
+  }
+
+  @AfterClass
+  public static void stopServices() throws Exception {
+    if (miniHS2 != null && miniHS2.isStarted()) {
+      miniHS2.stop();
+    }
+  }
+
+  @Test
+  public void testAccessDenied() throws Exception {
+    assertTrue("Test setup failed. MiniHS2 is not initialized",
+        miniHS2 != null && miniHS2.isStarted());
+
+    Class.forName(MiniHS2.getJdbcDriverName());
+    Path scratchDir = new Path(HiveConf.getVar(hiveConf, HiveConf.ConfVars.SCRATCHDIR));
+
+    MiniDFSShim dfs = miniHS2.getDfs();
+    FileSystem fs = dfs.getFileSystem();
+
+    Path stickyBitDir = new Path(scratchDir, "stickyBitDir");
+
+    fs.mkdirs(stickyBitDir);
+
+    String dataFileDir = hiveConf.get("test.data.files").replace('\\', '/')
+        .replace("c:", "").replace("C:", "").replace("D:", "").replace("d:", "");
+    Path dataFilePath = new Path(dataFileDir, "kv1.txt");
+
+    fs.copyFromLocalFile(dataFilePath, stickyBitDir);
+
+    FsPermission fsPermission = new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL, true);
+
+    // Sets the sticky bit on stickyBitDir - now removing file kv1.txt from stickyBitDir by
+    // unprivileged user will result in a DFS error.
+    fs.setPermission(stickyBitDir, fsPermission);
+
+    FileStatus[] files = fs.listStatus(stickyBitDir);
+
+    // Connecting to HS2 as foo.
+    Connection hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL(), "foo", "bar");
+    Statement stmt = hs2Conn.createStatement();
+
+    String tableName = "stickyBitTable";
+
+    stmt.execute("drop table if exists " + tableName);
+    stmt.execute("create table " + tableName + " (foo int, bar string)");
+
+    try {
+      // This statement will attempt to move kv1.txt out of stickyBitDir as user foo.  HS2 is
+      // expected to return 20009.
+      stmt.execute("LOAD DATA INPATH '" + stickyBitDir.toUri().getPath() + "/kv1.txt' "
+          + "OVERWRITE INTO TABLE " + tableName);
+    } catch (Exception e) {
+      if (e instanceof SQLException) {
+        SQLException se = (SQLException) e;
+        Assert.assertEquals("Unexpected error code", 20009, se.getErrorCode());
+        System.out.println(String.format("Error Message: %s", se.getMessage()));
+      } else
+        throw e;
+    }
+
+    stmt.execute("drop table if exists " + tableName);
+
+    stmt.close();
+    hs2Conn.close();
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 1190f9a..6471b90 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -1799,6 +1799,17 @@ public int execute(boolean deferClose) throws CommandNeedRetryException {
             }
             setErrorMsgAndDetail(exitVal, result.getTaskError(), tsk);
             SQLState = "08S01";
+
+            // 08S01 (Communication error) is the default sql state.  Override the sqlstate
+            // based on the ErrorMsg set in HiveException.
+            if (result.getTaskError() instanceof HiveException) {
+              ErrorMsg errorMsg = ((HiveException) result.getTaskError()).
+                  getCanonicalErrorMsg();
+              if (errorMsg != ErrorMsg.GENERIC_ERROR) {
+                SQLState = errorMsg.getSQLState();
+              }
+            }
+
             console.printError(errorMessage);
             driverCxt.shutdown();
             // in case we decided to run everything in local mode, restore the
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
index cd9ab91..724cb5f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -24,12 +24,22 @@
 import org.apache.hadoop.hive.ql.parse.ASTNode;
 import org.apache.hadoop.hive.ql.parse.ASTNodeOrigin;
 
+import java.io.FileNotFoundException;
 import java.text.MessageFormat;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import org.antlr.runtime.tree.Tree;
+import org.apache.hadoop.hdfs.protocol.DSQuotaExceededException;
+import org.apache.hadoop.hdfs.protocol.NSQuotaExceededException;
+import org.apache.hadoop.hdfs.protocol.UnresolvedPathException;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.ASTNodeOrigin;
+import org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableTypes;
+import org.apache.hadoop.security.AccessControlException;
+
 /**
  * List of all error messages.
  * This list contains both compile time and run-time errors.
@@ -448,6 +458,17 @@
   PARTITION_SCAN_LIMIT_EXCEEDED(20005, "Number of partitions scanned (={0}) on table {1} exceeds limit" +
       " (={2}). This is controlled by hive.limit.query.max.table.partition.", true),
 
+  // 20006-20008 used in upstream.
+
+  ACCESS_DENIED(20009, "Access denied: {0}", "42000", true),
+  QUOTA_EXCEEDED(20010, "Quota exceeded: {0}", "64000", true),
+  UNRESOLVED_PATH(20011, "Unresolved path: {0}", "64000", true),
+  FILE_NOT_FOUND(20012, "File not found: {0}", "64000", true),
+  WRONG_FILE_FORMAT(20013, "Wrong file format. Please check the file's format.", "64000", true),
+
+  // An exception from runtime that will show the full stack to client
+  UNRESOLVED_RT_EXCEPTION(29999, "Runtime Error: {0}", "58004", true),
+
   //========================== 30000 range starts here ========================//
   STATSPUBLISHER_NOT_OBTAINED(30000, "StatsPublisher cannot be obtained. " +
     "There was a error to retrieve the StatsPublisher, and retrying " +
@@ -525,6 +546,20 @@
   }
 
   /**
+   * Given a remote runtime exception, returns the ErrorMsg object associated with it.
+   * @param e An exception
+   * @return ErrorMsg
+   */
+  public static ErrorMsg getErrorMsg(Exception e) {
+    if (e instanceof AccessControlException) return ACCESS_DENIED;
+    if (e instanceof NSQuotaExceededException) return QUOTA_EXCEEDED;
+    if (e instanceof DSQuotaExceededException) return QUOTA_EXCEEDED;
+    if (e instanceof UnresolvedPathException) return UNRESOLVED_PATH;
+    if (e instanceof FileNotFoundException) return FILE_NOT_FOUND;
+    return UNRESOLVED_RT_EXCEPTION;
+  }
+
+  /**
    * Given an error message string, returns the ErrorMsg object associated with it.
    * @param mesg An error message string
    * @return ErrorMsg
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index 89704dc..0e78ad8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -33,6 +33,7 @@
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.DriverContext;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.mr.MapRedTask;
 import org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask;
 import org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer;
@@ -301,8 +302,7 @@ public int execute(DriverContext driverContext) {
             boolean flag = HiveFileFormatUtils.checkInputFormat(
                 srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);
             if (!flag) {
-              throw new HiveException(
-                  "Wrong file format. Please check the file's format.");
+              throw new HiveException(ErrorMsg.WRONG_FILE_FORMAT);
             }
           }
         }
@@ -496,6 +496,23 @@ public int execute(DriverContext driverContext) {
       }
 
       return 0;
+    } catch (HiveException he) {
+      int errorCode = 1;
+
+      if (he.getCanonicalErrorMsg() != ErrorMsg.GENERIC_ERROR) {
+        errorCode = he.getCanonicalErrorMsg().getErrorCode();
+        if (he.getCanonicalErrorMsg() == ErrorMsg.UNRESOLVED_RT_EXCEPTION) {
+          console.printError("Failed with exception " + he.getMessage(), "\n"
+              + StringUtils.stringifyException(he));
+        } else {
+          console.printError("Failed with exception " + he.getMessage()
+              + "\nRemote Exception: " + he.getRemoteErrorMsg());
+          console.printInfo("\n", StringUtils.stringifyException(he),false);
+        }
+      }
+
+      setException(he);
+      return errorCode;
     } catch (Exception e) {
       console.printError("Failed with exception " + e.getMessage(), "\n"
           + StringUtils.stringifyException(e));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
index 4da83d6..2b62ecb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
@@ -139,6 +139,7 @@
 import org.apache.thrift.TException;
 
 import com.google.common.collect.Sets;
+import com.google.common.base.Splitter;
 
 /**
  * This class has functions that implement meta data/DDL operations using calls
@@ -2599,6 +2600,9 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,
         final boolean needToCopy = needToCopy(srcP, destf, srcFs, destFs);
 
         final boolean isRenameAllowed = !needToCopy && !isSrcLocal;
+
+        final String msg = "Unable to move source " + srcP + " to destination " + destf;
+
         // If we do a rename for a non-local file, we will be transfering the original
         // file permissions from source to the destination. Else, in case of mvFile() where we
         // copy from source to destination, we will inherit the destination's parent group ownership.
@@ -2611,25 +2615,29 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,
             if (null != newFiles) {
               newFiles.add(destPath);
             }
-          } catch (IOException ioe) {
-            LOG.error(String.format("Failed to move: {}", ioe.getMessage()));
-            throw new HiveException(ioe.getCause());
+          } catch (Exception e) {
+            throw getHiveException(e, msg, "Failed to move: {}");
           }
         } else {
           futures.add(pool.submit(new Callable<ObjectPair<Path, Path>>() {
             @Override
-            public ObjectPair<Path, Path> call() throws Exception {
+            public ObjectPair<Path, Path> call() throws HiveException {
               SessionState.setCurrentSessionState(parentSession);
 
-              Path destPath = mvFile(conf, srcFs, srcP, destFs, destf, isSrcLocal, isRenameAllowed);
+              try {
+                Path destPath =
+                    mvFile(conf, srcFs, srcP, destFs, destf, isSrcLocal, isRenameAllowed);
 
-              if (inheritPerms) {
-                ShimLoader.getHadoopShims().setFullFileStatus(conf, fullDestStatus, srcGroup, destFs, destPath, false);
-              }
-              if (null != newFiles) {
-                newFiles.add(destPath);
+                if (inheritPerms) {
+                  ShimLoader.getHadoopShims().setFullFileStatus(conf, fullDestStatus, srcGroup, destFs, destPath, false);
+                }
+                if (null != newFiles) {
+                  newFiles.add(destPath);
+                }
+                return ObjectPair.create(srcP, destPath);
+              } catch (Exception e) {
+                throw getHiveException(e, msg);
               }
-              return ObjectPair.create(srcP, destPath);
             }
           }));
         }
@@ -2646,9 +2654,7 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,
           ObjectPair<Path, Path> pair = future.get();
           LOG.debug(String.format("Moved src: {}", pair.getFirst().toString(), ", to dest: {}", pair.getSecond().toString()));
         } catch (Exception e) {
-          LOG.error(String.format("Failed to move: {}", e.getMessage()));
-          pool.shutdownNow();
-          throw new HiveException(e.getCause());
+          throw handlePoolException(pool, e);
         }
       }
     }
@@ -2841,6 +2847,8 @@ public static boolean moveFile(final HiveConf conf, Path srcf, final Path destf,
     // (2) It is assumed that subdir and dir are in same encryption zone.
     // (3) Move individual files from scr dir to dest dir.
     boolean destIsSubDir = isSubDir(srcf, destf, srcFs, destFs, isSrcLocal);
+    final String msg = "Unable to move source " + srcf + " to destination " + destf;
+
     try {
       if (inheritPerms || replace) {
         try{
@@ -2891,6 +2899,10 @@ public static boolean moveFile(final HiveConf conf, Path srcf, final Path destf,
             for (final FileStatus srcStatus : srcs) {
 
               final Path destFile = new Path(destf, srcStatus.getPath().getName());
+
+              final String poolMsg =
+                  "Unable to move source " + srcStatus.getPath() + " to destination " + destFile;
+
               if (null == pool) {
                 if(!destFs.rename(srcStatus.getPath(), destFile)) {
                   throw new IOException("rename for src path: " + srcStatus.getPath() + " to dest:"
@@ -2899,16 +2911,21 @@ public static boolean moveFile(final HiveConf conf, Path srcf, final Path destf,
               } else {
                 futures.add(pool.submit(new Callable<Void>() {
                   @Override
-                  public Void call() throws Exception {
+                  public Void call() throws HiveException {
                     SessionState.setCurrentSessionState(parentSession);
                     final String group = srcStatus.getGroup();
-                    if(destFs.rename(srcStatus.getPath(), destFile)) {
-                      if (inheritPerms) {
-                        ShimLoader.getHadoopShims().setFullFileStatus(conf, desiredStatus, group, destFs, destFile, false);
+
+                    try {
+                      if(destFs.rename(srcStatus.getPath(), destFile)) {
+                        if (inheritPerms) {
+                          ShimLoader.getHadoopShims().setFullFileStatus(conf, desiredStatus, group, destFs, destFile, false);
+                        }
+                      } else {
+                        throw new IOException(
+                            "rename for src path: " + srcStatus.getPath() + " to dest path:" + destFile + " returned false");
                       }
-                    } else {
-                      throw new IOException("rename for src path: " + srcStatus.getPath() + " to dest path:"
-                          + destFile + " returned false");
+                    } catch (Exception e) {
+                      throw getHiveException(e, poolMsg);
                     }
                     return null;
                   }
@@ -2925,9 +2942,7 @@ public Void call() throws Exception {
                 try {
                   future.get();
                 } catch (Exception e) {
-                  LOG.debug(e.getMessage());
-                  pool.shutdownNow();
-                  throw new HiveException(e.getCause());
+                  throw handlePoolException(pool, e);
                 }
               }
             }
@@ -2953,11 +2968,59 @@ public Void call() throws Exception {
           }
         }
       }
-    } catch (IOException ioe) {
-      throw new HiveException("Unable to move source " + srcf + " to destination " + destf, ioe);
+    } catch (Exception e) {
+      throw getHiveException(e, msg);
     }
   }
 
+  static private HiveException getHiveException(Exception e, String msg) {
+    return getHiveException(e, msg, null);
+  }
+
+  static private HiveException handlePoolException(ExecutorService pool, Exception e) {
+    HiveException he = null;
+
+    if (e instanceof HiveException) {
+      he = (HiveException) e;
+      if (he.getCanonicalErrorMsg() != ErrorMsg.GENERIC_ERROR) {
+        if (he.getCanonicalErrorMsg() == ErrorMsg.UNRESOLVED_RT_EXCEPTION) {
+          LOG.error(String.format("Failed to move: {}", he.getMessage()));
+        } else {
+          LOG.info(String.format("Failed to move: {}", he.getRemoteErrorMsg()));
+        }
+      }
+    } else {
+      LOG.error(String.format("Failed to move: {}", e.getMessage()));
+      he = new HiveException(e.getCause());
+    }
+    pool.shutdownNow();
+    return he;
+  }
+
+  static private HiveException getHiveException(Exception e, String msg, String logMsg) {
+    // The message from remote exception includes the entire stack.  The error thrown from
+    // hive based on the remote exception needs only the first line.
+    String hiveErrMsg = null;
+
+    if (e.getMessage() != null) {
+      hiveErrMsg = String.format("%s%s%s", msg, ": ",
+          Splitter.on(System.getProperty("line.separator")).split(e.getMessage()).iterator()
+              .next());
+    } else {
+      hiveErrMsg = msg;
+    }
+
+    ErrorMsg errorMsg = ErrorMsg.getErrorMsg(e);
+
+    if (logMsg != null)
+      LOG.info(String.format(logMsg, e.getMessage()));
+
+    if (errorMsg != ErrorMsg.UNRESOLVED_RT_EXCEPTION)
+      return new HiveException(e, e.getMessage(), errorMsg, hiveErrMsg);
+    else
+      return new HiveException(msg, e);
+  }
+
   /**
    * If moving across different FileSystems or differnent encryption zone, need to do a File copy instead of rename.
    * TODO- consider if need to do this for different file authority.
@@ -3096,10 +3159,16 @@ private static void moveAcidFiles(FileSystem fs, FileStatus[] stats, Path dst,
             for (FileStatus bucketStat : bucketStats) {
               Path bucketSrc = bucketStat.getPath();
               Path bucketDest = new Path(deltaDest, bucketSrc.getName());
+              final String msg = "Unable to move source " + bucketSrc + " to destination " +
+                  bucketDest;
               LOG.info("Moving bucket " + bucketSrc.toUri().toString() + " to " +
                   bucketDest.toUri().toString());
-              fs.rename(bucketSrc, bucketDest);
-              if (newFiles != null) newFiles.add(bucketDest);
+              try {
+                fs.rename(bucketSrc, bucketDest);
+                if (newFiles != null) newFiles.add(bucketDest);
+              } catch (Exception e) {
+                throw getHiveException(e, msg);
+              }
             }
           } catch (IOException e) {
             throw new HiveException("Error moving acid files " + e.getMessage(), e);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveException.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveException.java
index 1d895ca..a23d8c0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveException.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveException.java
@@ -29,6 +29,12 @@
    * Standard predefined message with error code and possibly SQL State, etc.
    */
   private ErrorMsg canonicalErrorMsg = ErrorMsg.GENERIC_ERROR;
+
+  /**
+   * Error Messages returned from remote exception (eg. hadoop error)
+   */
+  private String remoteErrorMsg;
+
   public HiveException() {
     super();
   }
@@ -46,24 +52,40 @@ public HiveException(String message, Throwable cause) {
   }
 
   public HiveException(ErrorMsg message, String... msgArgs) {
-    this(null, message, msgArgs);
+    this(null, null, message, msgArgs);
+  }
+
+  public HiveException(Throwable cause, ErrorMsg errorMsg, String... msgArgs) {
+    this(cause, null, errorMsg, msgArgs);
+  }
+
+  public HiveException(Throwable cause, ErrorMsg errorMsg) {
+    this(cause, null, errorMsg, new String[0]);
+  }
+
+  public HiveException(ErrorMsg errorMsg) {
+    this(null, null, errorMsg, new String[0]);
   }
 
   /**
    * This is the recommended constructor to use since it helps use
-   * canonical messages throughout.  
+   * canonical messages throughout and propagate remote errors.
+   *
    * @param errorMsg Canonical error message
    * @param msgArgs message arguments if message is parametrized; must be {@code null} is message takes no arguments
    */
-  public HiveException(Throwable cause, ErrorMsg errorMsg, String... msgArgs) {
+  public HiveException(Throwable cause, String remErrMsg, ErrorMsg errorMsg, String... msgArgs) {
     super(errorMsg.format(msgArgs), cause);
     canonicalErrorMsg = errorMsg;
-
+    remoteErrorMsg = remErrMsg;
   }
+
   /**
    * @return {@link ErrorMsg#GENERIC_ERROR} by default
    */
   public ErrorMsg getCanonicalErrorMsg() {
     return canonicalErrorMsg;
   }
+
+  public String getRemoteErrorMsg() { return remoteErrorMsg; }
 }
diff --git a/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
index 6bdb4ee..ce08ae3 100644
--- a/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
+++ b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
@@ -633,7 +633,10 @@ public TGetOperationStatusResp GetOperationStatus(TGetOperationStatusReq req) th
       if (opException != null) {
         resp.setSqlState(opException.getSQLState());
         resp.setErrorCode(opException.getErrorCode());
-        resp.setErrorMessage(opException.getMessage());
+        if (opException.getErrorCode() == 29999)
+          resp.setErrorMessage(org.apache.hadoop.util.StringUtils.stringifyException(opException));
+        else
+          resp.setErrorMessage(opException.getMessage());
       }
       resp.setStatus(OK_STATUS);
     } catch (Exception e) {
-- 
1.7.9.5

