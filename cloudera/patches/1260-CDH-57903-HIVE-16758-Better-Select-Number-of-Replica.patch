From 2636c366e9722cad513e9999c9f50c47fa3e7540 Mon Sep 17 00:00:00 2001
From: BELUGA BEHR <dam6923@gmail.com>
Date: Tue, 8 Aug 2017 10:27:18 -0700
Subject: [PATCH 1260/1363] CDH-57903: HIVE-16758: Better Select Number of
 Replications (BELUGA BEHR, reviewed by Chao Sun)

Change-Id: I115e70c2056af9087dcf9608a1396d98c252ae96
---
 .../hive/ql/exec/SparkHashTableSinkOperator.java   |   17 +++++++----------
 1 file changed, 7 insertions(+), 10 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
index c75e231..20f9fbd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
@@ -49,13 +49,15 @@
   private final PerfLogger perfLogger = SessionState.getPerfLogger();
 
   protected static final Log LOG = LogFactory.getLog(SparkHashTableSinkOperator.class.getName());
-  public static final String DFS_REPLICATION_MAX = "dfs.replication.max";
-  private int minReplication = 10;
+
+  private static final String MAPRED_FILE_REPLICATION = "mapreduce.client.submit.file.replication";
+  private static final int DEFAULT_REPLICATION = 10;
 
   private HashTableSinkOperator htsOperator;
 
   // The position of this table
   private byte tag;
+  private short numReplication;
 
   public SparkHashTableSinkOperator() {
     htsOperator = new HashTableSinkOperator();
@@ -66,9 +68,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     ObjectInspector[] inputOIs = new ObjectInspector[conf.getTagLength()];
     inputOIs[tag] = inputObjInspectors[0];
     conf.setTagOrder(new Byte[]{ tag });
-    int dfsMaxReplication = hconf.getInt(DFS_REPLICATION_MAX, minReplication);
-    // minReplication value should not cross the value of dfs.replication.max
-    minReplication = Math.min(minReplication, dfsMaxReplication);
+    numReplication = (short) hconf.getInt(MAPRED_FILE_REPLICATION, DEFAULT_REPLICATION);
     htsOperator.setConf(conf);
     htsOperator.initialize(hconf, inputOIs);
   }
@@ -129,7 +129,6 @@ protected void flushToFile(MapJoinPersistableTableContainer tableContainer,
     String dumpFilePrefix = conf.getDumpFilePrefix();
     Path path = Utilities.generatePath(tmpURI, dumpFilePrefix, tag, fileName);
     FileSystem fs = path.getFileSystem(htsOperator.getConfiguration());
-    short replication = fs.getDefaultReplication(path);
 
     fs.mkdirs(path);  // Create the folder and its parents if not there
     while (true) {
@@ -144,9 +143,7 @@ protected void flushToFile(MapJoinPersistableTableContainer tableContainer,
         // No problem, use a new name
       }
     }
-    // TODO find out numOfPartitions for the big table
-    int numOfPartitions = replication;
-    replication = (short) Math.max(minReplication, numOfPartitions);
+
     htsOperator.console.printInfo(Utilities.now() + "\tDump the side-table for tag: " + tag
       + " with group count: " + tableContainer.size() + " into file: " + path);
     try {
@@ -155,7 +152,7 @@ protected void flushToFile(MapJoinPersistableTableContainer tableContainer,
       ObjectOutputStream out = null;
       MapJoinTableContainerSerDe mapJoinTableSerde = htsOperator.mapJoinTableSerdes[tag];
       try {
-        os = fs.create(path, replication);
+        os = fs.create(path, numReplication);
         out = new ObjectOutputStream(new BufferedOutputStream(os, 4096));
         mapJoinTableSerde.persist(out, tableContainer);
       } finally {
-- 
1.7.9.5

